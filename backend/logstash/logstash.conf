# ============================================================================
# LOGSTASH CONFIGURATION
# ============================================================================
# Logstash processes logs between Filebeat and Elasticsearch.
# It receives raw logs, parses them, extracts useful fields, and sends
# them to Elasticsearch in a structured format.
#
# PIPELINE: input → filter → output
# - Input: Receives logs from Filebeat
# - Filter: Parses JSON, extracts fields, cleans up data
# - Output: Sends processed logs to Elasticsearch
# ============================================================================

input {
  # Receive logs from Filebeat on port 5044
  beats {
    port => 5044
  }
}

filter {
  # ========================================================================
  # STEP 1: CLEAN UP DOCKER METADATA
  # ========================================================================
  # Remove unnecessary Docker metadata fields that clutter the logs.
  # We only keep what's useful for searching and filtering.
  # Remove unnecessary Docker metadata
  mutate {
    remove_field => [
      "[agent][ephemeral_id]",
      "[agent][id]",
      "[agent][hostname]",
      "[agent][name]",
      "[agent][type]",
      "[agent][version]",
      "[@version]"
    ]
  }
  
  # Remove Docker Compose label fields
  ruby {
    code => "
      begin
        fields_to_remove = []
        event.to_hash.keys.each do |key|
          key_str = key.to_s
          if key_str.include?('com_docker_compose') || 
             key_str.include?('com.docker.compose') ||
             key_str.match(/docker.*container.*labels.*compose/i) ||
             key_str.match(/container.*labels.*compose/i)
            fields_to_remove << key
          end
        end
        if event.include?('[container][labels]')
          labels = event.get('[container][labels]')
          if labels.is_a?(Hash)
            labels.keys.each do |label_key|
              label_key_str = label_key.to_s
              if label_key_str.include?('com_docker_compose') || 
                 label_key_str.include?('com.docker.compose')
                field_path = '[container][labels][' + label_key_str + ']'
                fields_to_remove << field_path
              end
            end
          end
        end
        if event.include?('[docker][container][labels]')
          labels = event.get('[docker][container][labels]')
          if labels.is_a?(Hash)
            labels.keys.each do |label_key|
              label_key_str = label_key.to_s
              if label_key_str.include?('com_docker_compose') || 
                 label_key_str.include?('com.docker.compose')
                field_path = '[docker][container][labels][' + label_key_str + ']'
                fields_to_remove << field_path
              end
            end
          end
        end
        fields_to_remove.each do |field|
          begin
            event.remove(field)
          rescue => e
          end
        end
      rescue => e
      end
    "
  }
  
  # ========================================================================
  # STEP 2: NORMALIZE SERVICE NAMES
  # ========================================================================
  # Convert container names like "auth_service" to "auth-service" for consistency.
  # This makes filtering easier in Kibana.
  if [container][name] {
    mutate {
      add_field => { "service" => "%{[container][name]}" }
      gsub => [ "service", "_service$", "" ]  # Remove "_service" suffix
      gsub => [ "service", "_", "-" ]          # Replace underscores with hyphens
    }
  }
  
  # ========================================================================
  # STEP 3: PARSE STRUCTURED JSON LOGS
  # ========================================================================
  # Our application uses Pino (a structured logger) that outputs JSON.
  # This section parses the JSON and extracts useful fields like:
  # - Log level, error codes, HTTP status, correlation IDs
  # - Request/response information
  # - Timestamps
  if [message] and [message] =~ /^\{.*\}$/ {
    json {
      source => "message"
      target => "log"
      tag_on_failure => ["_jsonparsefailure"]
    }
    
    # Remove string 'level' field if it exists (conflicts with Pino's numeric level)
    if [log][level] and [log][level] =~ /^[a-z]+$/ {
      mutate {
        remove_field => [ "[log][level]" ]
      }
    }
    
    # Extract structured logger fields (error_code, error_type, httpStatus)
    if [log][errorCode] {
      mutate {
        add_field => { "error_code" => "%{[log][errorCode]}" }
      }
    }
    if [log][errorType] {
      mutate {
        add_field => { "error_type" => "%{[log][errorType]}" }
      }
    }
    if [log][httpStatus] {
      mutate {
        add_field => { "http_status" => "%{[log][httpStatus]}" }
      }
    }
    if [log][correlationId] {
      mutate {
        add_field => { "correlation_id" => "%{[log][correlationId]}" }
      }
    }
    
    # Extract log level from Pino numeric levels
    if [log][level] {
      ruby {
        code => "
          level = event.get('[log][level]')
          if level.is_a?(Numeric)
            case level.to_i
            when 10
              event.set('log_level', 'trace')
            when 20
              event.set('log_level', 'debug')
            when 30
              event.set('log_level', 'info')
            when 40
              event.set('log_level', 'warn')
            when 50
              event.set('log_level', 'error')
            when 60
              event.set('log_level', 'fatal')
            else
              event.set('log_level', 'info')
            end
          elsif level.is_a?(String)
            event.set('log_level', level)
          end
        "
      }
    }
    
    # Extract message from Pino (uses 'msg' field)
    if [log][msg] {
      mutate {
        replace => { "message" => "%{[log][msg]}" }
      }
    }
    
    # Extract HTTP request info from Pino
    if [log][req] {
      if [log][req][method] {
        mutate {
          add_field => { "http_method" => "%{[log][req][method]}" }
        }
      }
      if [log][req][url] {
        mutate {
          add_field => { "http_url" => "%{[log][req][url]}" }
        }
      }
    }
    
    # Extract HTTP response status from Pino
    if [log][res] {
      if [log][res][statusCode] {
        mutate {
          add_field => { "http_status" => "%{[log][res][statusCode]}" }
        }
      }
    }
    
    # Parse timestamp from Pino
    if [log][time] {
      date {
        match => [ "[log][time]", "UNIX_MS" ]
        target => "@timestamp"
      }
    }
  }
  
  # ========================================================================
  # STEP 4: TAG ERRORS
  # ========================================================================
  # Add an "error" tag to error logs for easy filtering in Kibana.
  # This allows quick access to all error logs.
  if [log_level] == "error" or [error_code] {
    mutate {
      add_tag => ["error"]
    }
  }
}

# ============================================================================
# OUTPUT CONFIGURATION
# ============================================================================
# Send processed logs to Elasticsearch for storage and indexing.
# - Index name includes date (logs-2025.01.15) for organization
# - ILM policy automatically manages retention (deletes after 7 days)
output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    user => "elastic"
    password => "${ELASTIC_PASSWORD}"
    index => "logs-%{+YYYY.MM.dd}"  # Daily indices (logs-2025.01.15, etc.)
    ilm_enabled => true              # Enable Index Lifecycle Management
    ilm_policy => "logs-policy"      # Use our retention policy
  }
}
